{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "5yq5t1w0dUbt",
        "_afQqrq3dbNG",
        "M09stKnadllL",
        "KFWP7CKkq8xs",
        "gZGLvqnwqxao",
        "t1sBOC_6uCh9",
        "PydnYCe8vCNf",
        "4lAQVIDYjl1j",
        "qogw_tGOxSIQ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install and Import Required Libraries"
      ],
      "metadata": {
        "id": "5yq5t1w0dUbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mlflow dagshub"
      ],
      "metadata": {
        "id": "I8j_9OdDdUHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from tensorflow.keras import layers, models, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support, roc_auc_score, roc_curve, auc\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import mlflow\n",
        "import mlflow.tensorflow\n",
        "import dagshub\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from dagshub import upload_files\n",
        "from dagshub.data_engine import datasources\n",
        "import requests\n",
        "from sklearn.model_selection import KFold"
      ],
      "metadata": {
        "id": "TgEwepe1daT_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Configuration (Parameters)"
      ],
      "metadata": {
        "id": "_afQqrq3dbNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants for DagsHub\n",
        "REPO_OWNER = 'Jesteban247'\n",
        "REPO_NAME = 'COVID-19_CT_Scan_Classification_with_Transfer_Learning'"
      ],
      "metadata": {
        "id": "V6jVhtjzqUZq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants for Model Training\n",
        "EPOCHS = 25\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 0.001\n",
        "IMAGE_SIZE = (150, 150)\n",
        "CLASS_NAMES = ['1NonCOVID', '2COVID', '3CAP']"
      ],
      "metadata": {
        "id": "YzYgpsOSqX4i"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants for Data Directories\n",
        "DATA_DIR = '/content/large-covid19-ct-slice-dataset/curated_data/curated_data'\n",
        "PREPROCESSED_DIR = '/content/preprocessed_data'\n",
        "ARTIFACTS_DIR = '/content/artifacts'"
      ],
      "metadata": {
        "id": "ZytyRlZhdgcM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the artifacts directory exists\n",
        "os.makedirs(ARTIFACTS_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "B9_U1ax4qduP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DagsHub Setup\n",
        "def setup_dagshub():\n",
        "    \"\"\"Initialize DagsHub for MLflow logging if not already initialized.\"\"\"\n",
        "    if os.getenv('MLFLOW_TRACKING_URI'):\n",
        "        print(\"MLflow tracking is already set up. Skipping initialization.\")\n",
        "    else:\n",
        "        print(\"Initializing DagsHub for MLflow logging...\")\n",
        "        dagshub.init(repo_owner=REPO_OWNER, repo_name=REPO_NAME, mlflow=True)\n",
        "        print(\"DagsHub initialization completed.\")"
      ],
      "metadata": {
        "id": "fkWRY2K7visR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Download and Preprocessing"
      ],
      "metadata": {
        "id": "M09stKnadllL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To download the data from Kaggle, use the following function.\n",
        "\n",
        "You can find the dataset at this link:\n",
        "[Kaggle Large COVID-19 CT Slice Dataset.](https://www.kaggle.com/datasets/maedemaftouni/large-covid19-ct-slice-dataset/data)"
      ],
      "metadata": {
        "id": "bnFjBkZnrrYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_data():\n",
        "    \"\"\"Download and unzip the dataset from Kaggle.\"\"\"\n",
        "    print(\"Starting dataset download...\")\n",
        "    !kaggle datasets download -d maedemaftouni/large-covid19-ct-slice-dataset\n",
        "    zip_file_path = '/content/large-covid19-ct-slice-dataset.zip'\n",
        "    extract_dir = '/content/large-covid19-ct-slice-dataset/'\n",
        "\n",
        "    print(\"Extracting dataset...\")\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_dir)\n",
        "    print(\"Data downloaded and extracted successfully! \\n\")"
      ],
      "metadata": {
        "id": "VsYNVDYfrrDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function handles the complete pipeline for preparing the dataset. It loads the data, processes it by splitting into training and test sets, resizes images, and one-hot encodes labels. The preprocessed data is then saved for later use, streamlining the workflow and ensuring the data is ready for model training and evaluation."
      ],
      "metadata": {
        "id": "bhUMnxZksaf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data():\n",
        "    \"\"\"Load, preprocess, and save the dataset.\"\"\"\n",
        "    print(\"Loading and preprocessing dataset...\")\n",
        "    dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        directory=DATA_DIR,\n",
        "        labels='inferred',\n",
        "        label_mode='int',\n",
        "        class_names=CLASS_NAMES,\n",
        "        image_size=IMAGE_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    # Splitting dataset into training and test sets\n",
        "    num_batches = tf.data.experimental.cardinality(dataset).numpy()\n",
        "    split_index = int(num_batches * 0.7)\n",
        "    train_ds = dataset.take(split_index)\n",
        "    test_ds = dataset.skip(split_index)\n",
        "\n",
        "    def dataset_to_numpy(dataset):\n",
        "        \"\"\"Convert TensorFlow dataset to NumPy arrays.\"\"\"\n",
        "        images, labels = [], []\n",
        "        for img_batch, lbl_batch in tqdm(dataset, desc='Processing Dataset', unit='batch'):\n",
        "            images.append(img_batch.numpy())\n",
        "            labels.append(lbl_batch.numpy())\n",
        "        return tf.concat(images, axis=0), tf.concat(labels, axis=0)\n",
        "\n",
        "    # Converting datasets to NumPy arrays\n",
        "    train_images, train_labels = dataset_to_numpy(train_ds)\n",
        "    test_images, test_labels = dataset_to_numpy(test_ds)\n",
        "\n",
        "    # Resizing images and one-hot encoding labels\n",
        "    train_images = tf.image.resize(train_images, IMAGE_SIZE)\n",
        "    test_images = tf.image.resize(test_images, IMAGE_SIZE)\n",
        "    train_labels = to_categorical(train_labels, num_classes=len(CLASS_NAMES))\n",
        "    test_labels = to_categorical(test_labels, num_classes=len(CLASS_NAMES))\n",
        "\n",
        "    # Saving preprocessed data\n",
        "    os.makedirs(PREPROCESSED_DIR, exist_ok=True)\n",
        "    np.save(os.path.join(PREPROCESSED_DIR, 'train_images.npy'), train_images)\n",
        "    np.save(os.path.join(PREPROCESSED_DIR, 'train_labels.npy'), train_labels)\n",
        "    np.save(os.path.join(PREPROCESSED_DIR, 'test_images.npy'), test_images)\n",
        "    np.save(os.path.join(PREPROCESSED_DIR, 'test_labels.npy'), test_labels)\n",
        "    print(\"Data preprocessing and saving completed! \\n\")"
      ],
      "metadata": {
        "id": "CSHs22R4dswn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Alternative (Download Data from DagsHub)"
      ],
      "metadata": {
        "id": "KFWP7CKkq8xs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since I've already run the project and uploaded the preprocessed data to DagsHub, you can avoid downloading and preprocessing the data from Kaggle again. Instead, you can directly download the already processed data from DagsHub."
      ],
      "metadata": {
        "id": "MCLtgdNyrOlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_data_from_dagshub(data_source_name, save_dir=PREPROCESSED_DIR):\n",
        "    \"\"\"Download the preprocessed data from DagsHub and save it to the specified directory.\"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Access the data source from DagsHub\n",
        "    ds = datasources.get(f'{REPO_OWNER}/{REPO_NAME}', data_source_name)\n",
        "\n",
        "    # Query for all data points with size larger than 1 byte\n",
        "    query = ds[\"size\"] > 1\n",
        "    data_points = query.all().dataframe\n",
        "\n",
        "    # Iterate through each file in the dataframe and download it\n",
        "    for index, row in data_points.iterrows():\n",
        "        file_name = row['path']\n",
        "        file_url = row['dagshub_download_url']\n",
        "        file_size = int(row['size'])\n",
        "\n",
        "        # Prepare file path\n",
        "        file_path = os.path.join(save_dir, file_name)\n",
        "\n",
        "        # Download with a single progress bar per file\n",
        "        with requests.get(file_url, stream=True) as response, open(file_path, 'wb') as file:\n",
        "            total_size_in_bytes = int(response.headers.get('content-length', file_size))\n",
        "            block_size = 1024  # 1 Kibibyte\n",
        "            progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True, desc=f\"Downloading {file_name}\")\n",
        "\n",
        "            for data in response.iter_content(block_size):\n",
        "                file.write(data)\n",
        "                progress_bar.update(len(data))\n",
        "\n",
        "            progress_bar.close()\n",
        "\n",
        "        print(f\"Downloaded: {file_name} ({file_size} bytes)\")\n",
        "\n",
        "    print(\"Data download complete!\")"
      ],
      "metadata": {
        "id": "9rOpplo4rExN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data to variables"
      ],
      "metadata": {
        "id": "gZGLvqnwqxao"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function loads preprocessed dataset files with progress indicators. It reads the training and test images and labels from the specified directory, providing visual feedback on the loading process. Once all files are loaded, the function returns the datasets for further use.\n",
        "\n"
      ],
      "metadata": {
        "id": "RrhKZ45bskQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "    \"\"\"Load preprocessed data with progress indicators.\"\"\"\n",
        "    print(\"Loading preprocessed data...\")\n",
        "    with tqdm(total=4, desc=\"Loading Data\", unit=\"file\") as pbar:\n",
        "        train_images = np.load(os.path.join(PREPROCESSED_DIR, 'train_images.npy'))\n",
        "        pbar.update(1)\n",
        "        train_labels = np.load(os.path.join(PREPROCESSED_DIR, 'train_labels.npy'))\n",
        "        pbar.update(1)\n",
        "        test_images = np.load(os.path.join(PREPROCESSED_DIR, 'test_images.npy'))\n",
        "        pbar.update(1)\n",
        "        test_labels = np.load(os.path.join(PREPROCESSED_DIR, 'test_labels.npy'))\n",
        "        pbar.update(1)\n",
        "    print(\"\\nData loaded successfully! \\n\")\n",
        "    return train_images, train_labels, test_images, test_labels"
      ],
      "metadata": {
        "id": "ZQycDjSxqyGm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Building and Training"
      ],
      "metadata": {
        "id": "g5E3n5v2dt9L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "The `build_model` function constructs a Convolutional Neural Network (CNN) using VGG16 as a base model, applying transfer learning techniques. Here’s a detailed explanation of the process:\n",
        "\n",
        "1. **Transfer Learning with VGG16:**\n",
        "   - **VGG16 Overview:** VGG16 is a well-established deep learning model trained on the ImageNet dataset. It features 16 layers and excels in image classification tasks due to its effective feature extraction capabilities. For more on VGG16 and its use in transfer learning, refer to this [article](https://towardsdatascience.com/transfer-learning-with-vgg16-and-keras-50ea161580b4).\n",
        "   - **Feature Extraction:** In this approach, we use VGG16 as a feature extractor. Instead of training the model from scratch, we leverage the features learned by VGG16, which were derived from a large and diverse dataset.\n",
        "\n",
        "2. **Building the Model:**\n",
        "   - **Input Layer:** The model is designed to accept images of size 150x150 pixels with 3 color channels (RGB).\n",
        "   - **Base Model:** VGG16 is loaded with pre-trained weights from ImageNet, but without its final classification layers (`include_top=False`). This allows us to add our own custom classifier on top.\n",
        "   - **Freezing the Base Model:** We set `base_model.trainable = False` to prevent the weights of VGG16 from being updated during training. This ensures that the feature extraction capabilities of VGG16 remain unchanged while we train the new layers.\n",
        "   - **Custom Layers:**\n",
        "     - **Flatten:** Converts the 3D feature maps output by VGG16 into a 1D vector.\n",
        "     - **Dense Layers:** Two fully connected layers with ReLU activation functions are added to further process the extracted features. Dropout is applied to these layers to mitigate overfitting.\n",
        "     - **Output Layer:** A final dense layer with a softmax activation function produces class probabilities for three classes (adjustable depending on the specific classification task).\n",
        "\n",
        "3. **Compiling the Model:**\n",
        "   - **Optimizer:** The Adam optimizer is used with a defined learning rate to update model weights during training.\n",
        "   - **Loss Function:** Categorical crossentropy is employed as the loss function for multi-class classification.\n",
        "   - **Metrics:** Accuracy is monitored to evaluate model performance.\n",
        "\n",
        "4. **Saving Model Summary:**\n",
        "   - The architecture of the model, including layer details and parameters, is saved to a text file (`model_summary.txt`) in the `ARTIFACTS_DIR`. This file provides a comprehensive overview of the model structure.\n",
        "\n",
        "By utilizing VGG16 as a pre-trained base model, this function benefits from its robust feature extraction capabilities, while the additional layers tailor the model to the specific classification task at hand."
      ],
      "metadata": {
        "id": "0ycQQxXatTfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model():\n",
        "    \"\"\"Build the CNN model using VGG16 as the base and save the model summary.\"\"\"\n",
        "    print(\"Building CNN model with VGG16 as base...\")\n",
        "\n",
        "    # Define input and base model\n",
        "    inputs = Input(shape=(150, 150, 3))\n",
        "    base_model = VGG16(weights=\"imagenet\", include_top=False, input_tensor=inputs)\n",
        "    base_model.trainable = False\n",
        "\n",
        "    # Add custom layers on top of the base model\n",
        "    x = base_model.output\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(256, activation='relu')(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    predictions = layers.Dense(3, activation='softmax')(x)  # Assuming 3 classes\n",
        "\n",
        "    # Create the full model\n",
        "    model = models.Model(inputs=inputs, outputs=predictions)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Save the model summary to a text file in the artifacts folder\n",
        "    summary_file = os.path.join(ARTIFACTS_DIR, 'model_summary.txt')\n",
        "    with open(summary_file, 'w') as f:\n",
        "        model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
        "\n",
        "    print(\"Model built and summary saved. \\n\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "9CFzQxMftgHg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `train_model` function trains the provided CNN model with early stopping and returns the training history. Here’s an in-depth look at the function:\n",
        "\n",
        "1. **Early Stopping:**\n",
        "   - **Purpose:** To prevent overfitting and ensure that training stops when the model performance on the validation set no longer improves.\n",
        "   - **Configuration:** `EarlyStopping` is configured to monitor the validation accuracy (`val_accuracy`). It will stop training if no improvement is observed for 5 consecutive epochs (`patience=5`). Additionally, it restores the model weights from the epoch with the highest validation accuracy to ensure the best model is retained.\n",
        "\n",
        "2. **Model Training:**\n",
        "   - **Data Preparation:** The `preprocess_input` function is applied to the training images to ensure they are appropriately scaled for the VGG16 base model.\n",
        "   - **Training Process:**\n",
        "     - **Epochs:** The model is trained for a defined number of epochs (`EPOCHS`).\n",
        "     - **Validation Split:** 20% of the training data is set aside for validation to monitor model performance during training.\n",
        "     - **Batch Size:** The model processes data in batches of size `BATCH_SIZE` for efficiency.\n",
        "     - **Callbacks:** The `EarlyStopping` callback is used to manage training interruptions based on validation performance.\n",
        "\n",
        "3. **Completion and Return:**\n",
        "   - **Completion Message:** Prints a message indicating that model training is complete.\n",
        "   - **Return Value:** The function returns the `history` object, which contains detailed information about the training process, including metrics and losses for each epoch.\n",
        "\n",
        "This function ensures efficient and effective training of the model, leveraging early stopping to optimize performance and prevent overfitting."
      ],
      "metadata": {
        "id": "WJYDm-VXttHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_images, train_labels):\n",
        "    \"\"\"Train the model without early stopping and return the training history.\"\"\"\n",
        "    print(\"Training model...\")\n",
        "    history = model.fit(\n",
        "        preprocess_input(train_images),\n",
        "        train_labels,\n",
        "        epochs=EPOCHS,\n",
        "        validation_split=0.2,\n",
        "        batch_size=BATCH_SIZE\n",
        "    )\n",
        "    print(\"Model training completed. \\n\")\n",
        "    return history"
      ],
      "metadata": {
        "id": "6UlP-_FotxOO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model):\n",
        "    \"\"\"Save the trained model to the artifacts directory.\"\"\"\n",
        "    model_path = os.path.join(ARTIFACTS_DIR, \"covid19_ct_model.keras\")\n",
        "    model.save(model_path)\n",
        "    print(\"Model saved to artifacts directory.\\n\")"
      ],
      "metadata": {
        "id": "J2BjAI9Gt9Zo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation"
      ],
      "metadata": {
        "id": "t1sBOC_6uCh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(cm, cm_path):\n",
        "    \"\"\"Plot and save the confusion matrix to a file.\"\"\"\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.savefig(cm_path)\n",
        "    plt.close()\n",
        "    print(f\"Confusion matrix saved to {cm_path}\\n\")"
      ],
      "metadata": {
        "id": "7NdUOi6Qu6bw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `evaluate_model` function assesses the performance of the trained model on the test dataset. It calculates key metrics such as accuracy, loss, precision, recall, F1 score, and ROC-AUC. It also generates and saves a confusion matrix, which visualizes the model’s performance across different classes. Here’s a brief overview:\n",
        "\n",
        "1. **Model Predictions:** The function makes predictions on the test dataset and converts these into class labels.\n",
        "2. **Metric Calculation:** It computes precision, recall, F1 score, accuracy, and loss to evaluate model performance.\n",
        "3. **Confusion Matrix:** A confusion matrix is plotted and saved, providing a visual representation of classification results.\n",
        "4. **ROC-AUC Score:** This metric evaluates the model’s ability to distinguish between classes."
      ],
      "metadata": {
        "id": "bTIBRrnHurP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_images, test_labels):\n",
        "    \"\"\"Evaluate the model and save confusion matrix and other metrics.\"\"\"\n",
        "    print(\"Evaluating model...\")\n",
        "    y_pred = model.predict(preprocess_input(test_images))\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_true_classes = np.argmax(test_labels, axis=1)\n",
        "\n",
        "    # Calculate metrics\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_true_classes, y_pred_classes, average='weighted')\n",
        "    test_accuracy = np.mean(y_pred_classes == y_true_classes)\n",
        "    test_loss = model.evaluate(preprocess_input(test_images), test_labels, verbose=0)[0]\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "    cm_path = os.path.join(ARTIFACTS_DIR, 'confusion_matrix.png')\n",
        "    plot_confusion_matrix(cm, cm_path)\n",
        "\n",
        "    # ROC-AUC\n",
        "    roc_auc = roc_auc_score(test_labels, y_pred, average='weighted', multi_class='ovr')\n",
        "\n",
        "    print(f\"Test Accuracy: {test_accuracy}, Test Loss: {test_loss}, F1 Score: {f1}, ROC-AUC: {roc_auc}\")\n",
        "    print(\"Model evaluation completed. \\n\")\n",
        "    return test_accuracy, test_loss, precision, recall, f1, roc_auc"
      ],
      "metadata": {
        "id": "HG8EopdgdR-7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLflow Integration"
      ],
      "metadata": {
        "id": "PydnYCe8vCNf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `mlflow_tracking` function logs experiment details, including hyperparameters, key metrics, artifacts, and the trained model, to MLflow. This helps in tracking and managing machine learning experiments. It starts an MLflow run, records relevant data, and saves the model and artifacts for future reference."
      ],
      "metadata": {
        "id": "T8K89fNEvTbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mlflow_tracking(train_history, test_metrics, model):\n",
        "    \"\"\"Log metrics, artifacts, and model to MLflow.\"\"\"\n",
        "    print(\"Starting MLflow tracking...\")\n",
        "\n",
        "    with mlflow.start_run() as run:\n",
        "        # Log hyperparameters\n",
        "        mlflow.log_param(\"learning_rate\", LEARNING_RATE)\n",
        "        mlflow.log_param(\"epochs\", EPOCHS)\n",
        "        mlflow.log_param(\"batch_size\", BATCH_SIZE)\n",
        "\n",
        "        # Log important metrics\n",
        "        mlflow.log_metric(\"train_accuracy\", train_history.history['accuracy'][-1])\n",
        "        mlflow.log_metric(\"test_accuracy\", test_metrics[0])\n",
        "        mlflow.log_metric(\"test_loss\", test_metrics[1])\n",
        "        mlflow.log_metric(\"test_precision\", test_metrics[2])\n",
        "        mlflow.log_metric(\"test_recall\", test_metrics[3])\n",
        "        mlflow.log_metric(\"test_f1_score\", test_metrics[4])\n",
        "        mlflow.log_metric(\"test_roc_auc\", test_metrics[5])\n",
        "\n",
        "        # Log artifacts\n",
        "        mlflow.log_artifacts(ARTIFACTS_DIR)\n",
        "\n",
        "        # Log the model\n",
        "        mlflow.keras.log_model(\n",
        "            model,\n",
        "            artifact_path=\"model\",\n",
        "            registered_model_name=\"covid19_ct_model\"\n",
        "        )\n",
        "\n",
        "        print(\"MLflow tracking completed. \\n\")"
      ],
      "metadata": {
        "id": "95k125QJvCqC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extra (Upload Data to DagsHub)"
      ],
      "metadata": {
        "id": "4lAQVIDYjl1j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is how to uploa the preprocessed data to DagsHub"
      ],
      "metadata": {
        "id": "al0Yd3lvv33a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_data_to_dagshub():\n",
        "    \"\"\"Upload preprocessed data to DagsHub.\"\"\"\n",
        "    print(\"Uploading preprocessed data to DagsHub...\")\n",
        "    repo_path = f'{REPO_OWNER}/{REPO_NAME}'\n",
        "    data_path = PREPROCESSED_DIR\n",
        "    upload_files(repo_path, data_path)\n",
        "    print(\"Data uploaded to DagsHub successfully!\")"
      ],
      "metadata": {
        "id": "C3LQwwTrjiu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to Run the Code\n"
      ],
      "metadata": {
        "id": "kjpAPgucxOz8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation"
      ],
      "metadata": {
        "id": "qogw_tGOxSIQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To execute the entire workflow, follow these organized steps. Ensure you have the required functions defined before running the main script.\n",
        "\n",
        "1. **Set Up DagsHub:**\n",
        "   ```python\n",
        "   setup_dagshub()\n",
        "   ```\n",
        "   - Initializes the DagsHub environment for data versioning and management.\n",
        "\n",
        "2. **Download and Preprocess Data:**\n",
        "   ```python\n",
        "   download_data()\n",
        "   preprocess_data()\n",
        "   ```\n",
        "   - Downloads the raw data and preprocesses it for training.\n",
        "\n",
        "3. **Download Preprocessed Data from DagsHub:**\n",
        "   ```python\n",
        "   download_data_from_dagshub(data_source_name='preprocessed_data')\n",
        "   ```\n",
        "   - Retrieves preprocessed data if it has already been processed and uploaded to DagsHub.\n",
        "\n",
        "4. **Load Data:**\n",
        "   ```python\n",
        "   train_images, train_labels, test_images, test_labels = load_data()\n",
        "   ```\n",
        "   - Loads the preprocessed training and test data from local files.\n",
        "\n",
        "5. **Build and Train Model:**\n",
        "   ```python\n",
        "   model = build_model()\n",
        "   train_history = train_model(model, train_images, train_labels)\n",
        "   save_model(model)\n",
        "   ```\n",
        "   - Constructs the CNN model using VGG16, trains it with the training data, and saves the trained model.\n",
        "\n",
        "6. **Evaluate Model:**\n",
        "   ```python\n",
        "   test_metrics = evaluate_model(model, test_images, test_labels)\n",
        "   ```\n",
        "   - Assesses the model’s performance on the test data and calculates metrics.\n",
        "\n",
        "7. **Track with MLflow:**\n",
        "   ```python\n",
        "   mlflow_tracking(train_history, test_metrics, model)\n",
        "   ```\n",
        "   - Logs metrics, artifacts, and the model to MLflow for tracking and management.\n",
        "\n",
        "8. **Upload Data to DagsHub:**\n",
        "   ```python\n",
        "   upload_data_to_dagshub()\n",
        "   ```\n",
        "   - Uploads the processed data and model artifacts to DagsHub.\n",
        "\n"
      ],
      "metadata": {
        "id": "4820jQlKwhKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code"
      ],
      "metadata": {
        "id": "Yl-CgnhdxVfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "setup_dagshub()"
      ],
      "metadata": {
        "id": "QuWSmVB3hlv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_data()\n",
        "preprocess_data()"
      ],
      "metadata": {
        "id": "xgNRJzD0n1it"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_data_from_dagshub(data_source_name='preprocessed_data')"
      ],
      "metadata": {
        "id": "IzL02HLGkJIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images, train_labels, test_images, test_labels = load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckc0zNPaxBSY",
        "outputId": "68f5013c-054d-472b-95b1-3a7a2a902cce"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading preprocessed data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading Data: 100%|██████████| 4/4 [00:18<00:00,  4.67s/file]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Data loaded successfully! \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model()\n",
        "train_history = train_model(model, train_images, train_labels)\n",
        "save_model(model)"
      ],
      "metadata": {
        "id": "6bMAw_9bxEFW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fe543be9-f890-4162-b29c-ce8d47b8a6c7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building CNN model with VGG16 as base...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model built and summary saved. \n",
            "\n",
            "Training model...\n",
            "Epoch 1/25\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 287ms/step - accuracy: 0.6492 - loss: 4.0077 - val_accuracy: 0.8438 - val_loss: 0.3976\n",
            "Epoch 2/25\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 163ms/step - accuracy: 0.8087 - loss: 0.4839 - val_accuracy: 0.8571 - val_loss: 0.3484\n",
            "Epoch 3/25\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 164ms/step - accuracy: 0.8457 - loss: 0.4026 - val_accuracy: 0.8789 - val_loss: 0.3041\n",
            "Epoch 4/25\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 164ms/step - accuracy: 0.8496 - loss: 0.3803 - val_accuracy: 0.9018 - val_loss: 0.2440\n",
            "Epoch 5/25\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 163ms/step - accuracy: 0.8755 - loss: 0.3091 - val_accuracy: 0.9031 - val_loss: 0.2757\n",
            "Epoch 6/25\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 164ms/step - accuracy: 0.8801 - loss: 0.3116 - val_accuracy: 0.9194 - val_loss: 0.2193\n",
            "Epoch 7/25\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 164ms/step - accuracy: 0.8878 - loss: 0.2821 - val_accuracy: 0.9089 - val_loss: 0.2448\n",
            "Epoch 8/25\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - accuracy: 0.8905 - loss: 0.2790 - val_accuracy: 0.9294 - val_loss: 0.1772\n",
            "Epoch 9/25\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 165ms/step - accuracy: 0.9030 - loss: 0.2631 - val_accuracy: 0.9169 - val_loss: 0.2305\n",
            "Epoch 10/25\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 165ms/step - accuracy: 0.8963 - loss: 0.2731 - val_accuracy: 0.9140 - val_loss: 0.2118\n",
            "Epoch 11/25\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 163ms/step - accuracy: 0.9002 - loss: 0.2652 - val_accuracy: 0.9323 - val_loss: 0.1788\n",
            "Epoch 12/25\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 164ms/step - accuracy: 0.9153 - loss: 0.2265 - val_accuracy: 0.9319 - val_loss: 0.1788\n",
            "Epoch 13/25\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 166ms/step - accuracy: 0.9231 - loss: 0.2054 - val_accuracy: 0.9282 - val_loss: 0.2078\n",
            "Epoch 14/25\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 164ms/step - accuracy: 0.9238 - loss: 0.2109 - val_accuracy: 0.9386 - val_loss: 0.1798\n",
            "Epoch 15/25\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 165ms/step - accuracy: 0.9268 - loss: 0.1976 - val_accuracy: 0.9336 - val_loss: 0.1765\n",
            "Epoch 16/25\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 166ms/step - accuracy: 0.9314 - loss: 0.1786 - val_accuracy: 0.9344 - val_loss: 0.1621\n",
            "Epoch 17/25\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 161ms/step - accuracy: 0.9329 - loss: 0.1761 - val_accuracy: 0.9461 - val_loss: 0.1655\n",
            "Epoch 18/25\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 166ms/step - accuracy: 0.9302 - loss: 0.1900 - val_accuracy: 0.9373 - val_loss: 0.1680\n",
            "Epoch 19/25\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 163ms/step - accuracy: 0.9135 - loss: 0.2223 - val_accuracy: 0.9323 - val_loss: 0.1573\n",
            "Epoch 20/25\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - accuracy: 0.9306 - loss: 0.1891 - val_accuracy: 0.9411 - val_loss: 0.1829\n",
            "Epoch 21/25\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 166ms/step - accuracy: 0.9276 - loss: 0.1858 - val_accuracy: 0.9432 - val_loss: 0.1544\n",
            "Epoch 22/25\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 164ms/step - accuracy: 0.9313 - loss: 0.1783 - val_accuracy: 0.9403 - val_loss: 0.1574\n",
            "Epoch 23/25\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 163ms/step - accuracy: 0.9309 - loss: 0.1772 - val_accuracy: 0.9536 - val_loss: 0.1454\n",
            "Epoch 24/25\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 164ms/step - accuracy: 0.9402 - loss: 0.1555 - val_accuracy: 0.9532 - val_loss: 0.1338\n",
            "Epoch 25/25\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 163ms/step - accuracy: 0.9462 - loss: 0.1375 - val_accuracy: 0.9553 - val_loss: 0.1316\n",
            "Model training completed. \n",
            "\n",
            "Model saved to artifacts directory.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_metrics = evaluate_model(model, test_images, test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0TNq7lLxFXk",
        "outputId": "fa401dec-c189-4534-86f2-18dc6b58b496"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model...\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 92ms/step\n",
            "Confusion matrix saved to /content/artifacts/confusion_matrix.png\n",
            "\n",
            "Test Accuracy: 0.9524922118380063, Test Loss: 0.36015427112579346, F1 Score: 0.9525053629110724, ROC-AUC: 0.9895720547519103\n",
            "Model evaluation completed. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mlflow_tracking(train_history, test_metrics, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWWIkvkExGbA",
        "outputId": "e5b772c2-2a60-4f71-d1c6-55ba3351b9cf"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting MLflow tracking...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024/08/24 03:10:04 WARNING mlflow.keras.save: You are saving a Keras model without specifying model signature.\n",
            "/usr/local/lib/python3.10/dist-packages/_distutils_hack/__init__.py:32: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
            "  warnings.warn(\n",
            "Successfully registered model 'covid19_ct_model'.\n",
            "Created version '1' of model 'covid19_ct_model'.\n",
            "2024/08/24 03:10:17 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLflow tracking completed. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "upload_data_to_dagshub()"
      ],
      "metadata": {
        "id": "kwAYbJ0bxHmR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}